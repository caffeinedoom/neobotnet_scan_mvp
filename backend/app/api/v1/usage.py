"""
LEAN Usage API - Optimized Reconnaissance Data

This module provides the /api/v1/usage/recon-data endpoint that the frontend
expects. Uses optimized batch queries for performance.

LEAN ARCHITECTURE: All authenticated users see ALL data (no user filtering).

PERFORMANCE FIX (2025-01-02):
- OLD: 72+ N+1 queries taking 15-30 seconds
- NEW: 8 batch queries taking <1 second

PERFORMANCE FIX v2 (2025-01-02):
- OLD: 75 sequential count queries taking ~10 seconds
- NEW: Parallel queries with asyncio taking ~0.5-1 second
"""
from fastapi import APIRouter, Depends, HTTPException, status
from typing import Dict, Any, List
import logging
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor

from ...core.dependencies import get_current_user
from ...schemas.auth import UserResponse
from ...core.supabase_client import supabase_client

router = APIRouter(prefix="/usage", tags=["usage"])
logger = logging.getLogger(__name__)


@router.get("/recon-data", response_model=Dict[str, Any])
async def get_recon_data(
    current_user: UserResponse = Depends(get_current_user)
):
    """
    Get unified reconnaissance data for the dashboard.
    
    LEAN Architecture: All authenticated users see ALL data.
    OPTIMIZED: Uses batch queries instead of N+1 pattern.
    
    Returns:
        summary: Overview statistics
        assets: List of assets with stats
        recent_scans: Recent scan jobs
    """
    start_time = time.time()
    
    try:
        client = supabase_client.service_client
        user_id = current_user.id
        
        logger.info(f"üöÄ Fetching recon-data (LEAN architecture) for user {user_id}")
        
        # ================================================================
        # BATCH QUERY 1: Use asset_overview view (has pre-computed subdomain counts)
        # This view already JOINs assets -> asset_scan_jobs -> subdomains
        # and computes domain_count, subdomain_count, active_domain_count
        # ================================================================
        assets_result = client.table("asset_overview").select(
            "id, name, description, bug_bounty_url, is_active, priority, tags, created_at, updated_at, domain_count, subdomain_count, active_domain_count"
        ).order("created_at", desc=True).execute()
        
        assets = assets_result.data or []
        asset_ids = [a["id"] for a in assets]
        
        if not asset_ids:
            logger.info("No assets found in database")
            return {
                "summary": {
                    "total_assets": 0,
                    "active_assets": 0,
                    "total_domains": 0,
                    "active_domains": 0,
                    "total_scans": 0,
                    "completed_scans": 0,
                    "failed_scans": 0,
                    "pending_scans": 0,
                    "total_subdomains": 0,
                    "total_probes": 0,
                    "total_dns_records": 0,
                    "total_urls": 0,
                    "last_scan_date": None
                },
                "assets": [],
                "recent_scans": []
            }
        
        # Pre-compute subdomain counts from view data
        asset_subdomain_counts = {a["id"]: a.get("subdomain_count", 0) for a in assets}
        domain_counts = {a["id"]: a.get("domain_count", 0) for a in assets}
        active_domain_counts = {a["id"]: a.get("active_domain_count", 0) for a in assets}
        
        total_domains = sum(domain_counts.values())
        active_domains = sum(active_domain_counts.values())
        total_subdomains = sum(asset_subdomain_counts.values())
        
        # ================================================================
        # BATCH QUERY 2: Get ALL scan jobs with status
        # ================================================================
        scans_result = client.table("asset_scan_jobs").select(
            "id, asset_id, status, modules, total_domains, completed_domains, created_at, started_at, completed_at, error_message"
        ).in_("asset_id", asset_ids).order("created_at", desc=True).execute()
        
        scans_data = scans_result.data or []
        scan_job_ids = [s["id"] for s in scans_data]
        
        # Pre-compute scan stats per asset
        scan_stats = {}
        for s in scans_data:
            aid = s["asset_id"]
            if aid not in scan_stats:
                scan_stats[aid] = {"total": 0, "completed": 0, "failed": 0, "pending": 0, "last_scan": None}
            scan_stats[aid]["total"] += 1
            status = s.get("status", "")
            if status == "completed":
                scan_stats[aid]["completed"] += 1
            elif status == "failed":
                scan_stats[aid]["failed"] += 1
            elif status in ["pending", "running"]:
                scan_stats[aid]["pending"] += 1
            # Track last scan date
            if scan_stats[aid]["last_scan"] is None or s["created_at"] > scan_stats[aid]["last_scan"]:
                scan_stats[aid]["last_scan"] = s["created_at"]
        
        # Global scan stats
        total_scans = len(scans_data)
        completed_scans = len([s for s in scans_data if s.get("status") == "completed"])
        failed_scans = len([s for s in scans_data if s.get("status") == "failed"])
        pending_scans = len([s for s in scans_data if s.get("status") in ["pending", "running"]])
        last_scan_date = scans_data[0]["created_at"] if scans_data else None
        
        # ================================================================
        # BATCH QUERIES 3-5: Get per-asset HTTP probe, DNS record, and URL counts
        # OPTIMIZED: Run all 75 queries in PARALLEL (was sequential, taking ~10s)
        # Now takes ~0.5-1s by using asyncio + ThreadPoolExecutor
        # ================================================================
        
        # Helper functions for parallel execution
        def get_probe_count(aid: str) -> tuple:
            result = client.table("http_probes").select("id", count="exact").eq("asset_id", aid).limit(1).execute()
            return (aid, "probe", result.count or 0)
        
        def get_dns_count(aid: str) -> tuple:
            result = client.table("dns_records").select("id", count="exact").eq("asset_id", aid).limit(1).execute()
            return (aid, "dns", result.count or 0)
        
        def get_url_count(aid: str) -> tuple:
            result = client.table("urls").select("id", count="exact").eq("asset_id", aid).limit(1).execute()
            return (aid, "url", result.count or 0)
        
        # Run all count queries in parallel using ThreadPoolExecutor
        asset_probe_counts = {}
        asset_dns_counts = {}
        asset_url_counts = {}
        
        # Use get_running_loop() for compatibility with FastAPI's event loop
        loop = asyncio.get_running_loop()
        with ThreadPoolExecutor(max_workers=25) as executor:
            # Create all tasks (75 total: 25 assets √ó 3 count types)
            futures = []
            for aid in asset_ids:
                futures.append(loop.run_in_executor(executor, get_probe_count, aid))
                futures.append(loop.run_in_executor(executor, get_dns_count, aid))
                futures.append(loop.run_in_executor(executor, get_url_count, aid))
            
            # Wait for all to complete
            results = await asyncio.gather(*futures)
        
        # Process results
        for aid, count_type, count in results:
            if count_type == "probe":
                asset_probe_counts[aid] = count
            elif count_type == "dns":
                asset_dns_counts[aid] = count
            elif count_type == "url":
                asset_url_counts[aid] = count
        
        total_probes = sum(asset_probe_counts.values())
        total_dns_records = sum(asset_dns_counts.values())
        total_urls = sum(asset_url_counts.values())
        
        # ================================================================
        # Build enriched assets (using pre-computed data from asset_overview view)
        # ================================================================
        enriched_assets = []
        for asset in assets:
            aid = asset["id"]
            stats = scan_stats.get(aid, {"total": 0, "completed": 0, "failed": 0, "pending": 0, "last_scan": None})
            
            enriched_assets.append({
                "id": aid,
                "name": asset["name"],
                "description": asset.get("description"),
                "bug_bounty_url": asset.get("bug_bounty_url"),
                "is_active": asset.get("is_active", True),
                "priority": asset.get("priority", 0),
                "tags": asset.get("tags", []),
                "created_at": asset["created_at"],
                "updated_at": asset["updated_at"],
                "apex_domain_count": domain_counts.get(aid, 0),
                "active_domain_count": active_domain_counts.get(aid, 0),
                "total_scans": stats["total"],
                "completed_scans": stats["completed"],
                "failed_scans": stats["failed"],
                "pending_scans": stats["pending"],
                "total_subdomains": asset_subdomain_counts.get(aid, 0),  # From asset_overview view
                "total_probes": asset_probe_counts.get(aid, 0),  # HTTP probes = live servers
                "total_dns_records": asset_dns_counts.get(aid, 0),
                "total_urls": asset_url_counts.get(aid, 0),  # Discovered URLs from crawlers
                "last_scan_date": stats["last_scan"]
            })
        
        # ================================================================
        # BATCH QUERY 5: Get subdomain counts for recent scans (only top 20)
        # OPTIMIZED: Run all 20 queries in PARALLEL
        # ================================================================
        recent_scan_ids = [s["id"] for s in scans_data[:20]]
        scan_subdomain_counts = {}
        
        if recent_scan_ids:
            def get_scan_subdomain_count(scan_id: str) -> tuple:
                result = client.table("subdomains").select("id", count="exact").eq("scan_job_id", scan_id).limit(1).execute()
                return (scan_id, result.count or 0)
            
            # Run scan subdomain count queries in parallel
            with ThreadPoolExecutor(max_workers=20) as executor:
                scan_futures = [loop.run_in_executor(executor, get_scan_subdomain_count, sid) for sid in recent_scan_ids]
                scan_results = await asyncio.gather(*scan_futures)
            
            for scan_id, count in scan_results:
                scan_subdomain_counts[scan_id] = count
        
        # ================================================================
        # Build recent scans (limit to 20)
        # ================================================================
        recent_scans = []
        asset_name_map = {a["id"]: a["name"] for a in assets}
        
        for scan in scans_data[:20]:
            status_val = scan.get("status", "pending")
            progress = 0
            if status_val == "completed":
                progress = 100
            elif status_val == "running":
                td = scan.get("total_domains", 0)
                cd = scan.get("completed_domains", 0)
                progress = int((cd / td) * 100) if td > 0 else 50
            
            recent_scans.append({
                "id": scan["id"],
                "asset_id": scan["asset_id"],
                "asset_name": asset_name_map.get(scan["asset_id"], "Unknown"),
                "status": status_val,
                "modules": scan.get("modules", []),
                "total_domains": scan.get("total_domains", 0),
                "completed_domains": scan.get("completed_domains", 0),
                "active_domains_only": True,
                "created_at": scan["created_at"],
                "started_at": scan.get("started_at"),
                "completed_at": scan.get("completed_at"),
                "estimated_completion": None,
                "error_message": scan.get("error_message"),
                "progress_percentage": progress,
                "subdomains_found": scan_subdomain_counts.get(scan["id"], 0),
                "scan_type": "reconnaissance"
            })
        
        # ================================================================
        # Build summary
        # ================================================================
        summary = {
            "total_assets": len(enriched_assets),
            "active_assets": len([a for a in enriched_assets if a.get("is_active", True)]),
            "total_domains": total_domains,
            "active_domains": active_domains,
            "total_scans": total_scans,
            "completed_scans": completed_scans,
            "failed_scans": failed_scans,
            "pending_scans": pending_scans,
            "total_subdomains": total_subdomains,
            "total_probes": total_probes,  # HTTP probes = live servers
            "total_dns_records": total_dns_records,
            "total_urls": total_urls,  # Discovered URLs from crawlers
            "last_scan_date": last_scan_date
        }
        
        elapsed_ms = (time.time() - start_time) * 1000
        logger.info(
            f"‚úÖ Recon-data returned in {elapsed_ms:.0f}ms: "
            f"{len(enriched_assets)} assets, {total_subdomains} subdomains, {total_scans} scans"
        )
        
        return {
            "summary": summary,
            "assets": enriched_assets,
            "recent_scans": recent_scans
        }
        
    except Exception as e:
        elapsed_ms = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Error getting recon-data after {elapsed_ms:.0f}ms: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get reconnaissance data"
        )

